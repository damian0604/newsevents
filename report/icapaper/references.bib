@article{Beam2009,
abstract = {A panel study of 400 U.S. journalists assessed changes in indicators of professionalism between 2002 and 2007, a period of significant economic and technological turmoil for news organizations. Findings show that professional organization membership declined among journalists, and staff cutbacks and higher workloads posed threats to the autonomy of some news workers. Beliefs about professional roles shifted slightly, with more emphasis on analyzing problems and being adversaries of public officials. Finally, journalists became more ethically cautious during the five-year span of the study, a period in which ethical lapses were disclosed by several high-profile news organizations. ABSTRACT FROM AUTHOR},
author = {Beam, Randal A and Weaver, David H and Brownlee, Bonnie J},
file = {:Users/damian/Dropbox/uva/literatuur-mendeley/Beam, Weaver, Brownlee{\_}2009.pdf:pdf},
issn = {10776990},
journal = {Journalism and Mass Communication Quarterly},
number = {2},
pages = {277--298},
publisher = {Association for Education in Journalism Mass Communication},
title = {{Changes in professionalism of U.S. journalists in the turbulent twenty-first century}},
volume = {86},
year = {2009}
}

@article{Eilders2006,
author = {Eilders, Christiane},
doi = {10.1515/COMMUN.2006.002},
file = {:Users/damian/Dropbox/uva/literatuur-mendeley/Eilders{\_}2006.pdf:pdf},
issn = {0341-2059},
journal = {Communications},
keywords = {content analysis,news factors,news retention,news selection},
number = {1},
pages = {5--24},
title = {{News factors and news decisions. Theoretical and methodological advances in Germany}},
volume = {31},
year = {2006}
}


@article{Harcup2016,
author = {Harcup, Tony and Neill, Deirdre O},
doi = {10.1080/1461670X.2016.1150193},
file = {:Users/damian/Dropbox/uva/literatuur-mendeley/Harcup, Neill{\_}2016.pdf:pdf},
issn = {1461-670X},
journal = {Journalism Studies},
number = {May},
pages = {1--19},
title = {{What is news?}},
volume = {9699},
year = {2016}
}

@phdthesis{Trilling2013a,
abstract = {2013, June},
author = {Trilling, Damian},
file = {:Users/damian/Dropbox/uva/literatuur-mendeley/Trilling{\_}2013.pdf:pdf},
school = {University of Amsterdam},
title = {{Following the news: Patterns of online and offline news consumption}},
type = {PhD dissertation},
year = {2013}
}


@inproceedings{INCA,
author = {Trilling, Damian and {Van De Velde}, Bob and Kroon, Anne C and Locherbach, Felicia and Araujo, Theo and Strycharz, Joanna and Raats, Tamara and {De Klerk}, Lisa and Jonkman, Jeroen G.F.},
booktitle = {{2018 IEEE 14th International Conference on e-Science (e-Science)}},
doi = {10.1109/eScience.2018.00078},
file = {:Users/damian/Dropbox/uva/literatuur-mendeley/Trilling et al.{\_}2018.pdf:pdf},
isbn = {978-1-5386-9156-4},
pages = {329--330},
OPTpublisher = {IEEE},
title = {{INCA: Infrastructure for Content Analysis}},
year = {2018}
}


@book{deburgh2008,
  title={Investigative journalism: Context and practice},
  editor={De Burgh, Hugo},
  year={2008},
  address = {New York, NY},
  publisher={Routledge}
}

@book{boczkowski2013,
title = {The news gap:{W}hen the information preferences of the media and the public diverge},
author = {Pablo J. Boczkowski and Eugenia Mitchelstein},
address = {Cambridge, MA},
publisher = {MIT},
year = {2013}
}

@article{Welbers2016,
author = {Welbers, Kasper and van Atteveldt, Wouter and Kleinnijenhuis, Jan and Ruigrok, Nel},
doi = {10.1080/1461670X.2016.1190663},
file = {:Users/damian/Dropbox/uva/literatuur-mendeley/Welbers et al.{\_}2018.pdf:pdf},
issn = {1461-670X},
journal = {Journalism Studies},
number = {3},
pages = {315--333},
title = {A Gatekeeper among Gatekeepers},
volume = {19},
year = {2018}
}

@article{Boumans2018,
abstract = {While it is generally acknowledged that news agencies play a pivotal role in the current news landscape, empirical insights into the extent of news media's reliance on agency copy are scarce. This study applies an innovative automated approach to trace agency copy for an entire year (n = 119,452) in the major print and online news media articles (n = 247,161) of the Dutch news landscape. Results suggest that particularly online news is highly dependent on agency content, with the agency being responsible for up to 75{\%} of the online news articles. Furthermore, a large part of the online news consists of verbatim agency copy, involving little or no editing. The results provide a strong rationale to place news agencies high on the agenda of news production scholars. Moreover, the demonstrated agency domination of online news is alarming in the context of news diversity},
author = {Boumans, Jelle and Trilling, Damian and Vliegenthart, Rens and Boomgaarden, Hajo},
file = {:Users/damian/Dropbox/uva/literatuur-mendeley/Boumans et al.{\_}2018.pdf:pdf},
journal = {International Journal of Communication},
keywords = {automated content analysis,churnalism,intermedia agenda setting,news agency,news production,online,print,transparency},
pages = {1768--1789},
title = {The Agency Makes the (Online) News World Go Round: {T}he Impact of News Agency Content on Print and Online News},
volume = {12},
year = {2018}
}

@incollection{mccombs2009,
address = {New York, NY},
author = {McCombs, M. and Reynolds, A.},
booktitle = {Media effects: Advances in theory and research},
editor = {J. Bryant and M. B. Oliver},
isbn = {0-89859-585-1},
pages = {1--16},
publisher = {Routledge},
title = {How the news shapes our civic agenda},
year = {1985}
}

@article{Nicholls2018,
abstract = {Content analysis of news stories (whether manual or automatic) is a cornerstone of the communication studies field. However, much research is conducted at the level of individual news articles, despite the fact that news events (especially significant ones) are frequently presented as "stories" by news outlets: chains of connected articles covering the same event from different angles. These stories are theoretically highly important in terms of increasing public recall of news items and enhancing the agenda-setting power of the press. Yet thus far, the field has lacked an efficient method for detecting groups of articles which form stories in a way that enables their analysis. In this work, we present a novel, automated method for identifying linked news stories from within a corpus of articles. This method makes use of techniques drawn from the field of information retrieval to identify textual closeness of pairs of articles, and then clustering techniques taken from the field of network analysis to group these articles into stories. We demonstrate the application of the method to a corpus of 61,864 articles, and show how it can efficiently identify valid story clusters within the corpus. We use the results to make observations about the prevalence and dynamics of stories within the UK news media, showing that more than 50{\%} of news production takes place within stories.},
author = {Nicholls, Tom and Bright, Jonathan},
doi = {10.1080/19312458.2018.1536972},
file = {:home/damian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Nicholls, Bright - 2018 - Understanding News Story Chains using Information Retrieval and Network Clustering Techniques.pdf:pdf},
issn = {1931-2458},
journal = {Communication Methods and Measures},
number = {1},
pages = {43--59},
title = {Understanding News Story Chains using Information Retrieval and Network Clustering Techniques},
volume = {13},
year = {2019}
}




@article{Vossen2016,
abstract = {In this article, we describe a system that reads news articles in four different languages and detects what happened, who is involved, where and when. This event-centric information is represented as episodic situational knowledge on individuals in an interoperable RDF format that allows for reasoning on the implications of the events. Our system covers the complete path from unstructured text to structured knowledge, for which we defined a formal model that links interpreted textual mentions of things to their representation as instances. The model forms the skeleton for interoperable interpretation across different sources and languages. The real content, however, is defined using multilingual and cross-lingual knowledge resources, both semantic and episodic. We explain how these knowledge resources are used for the processing of text and ultimately define the actual content of the episodic situational knowledge that is reported in the news. The knowledge and model in our system can be seen as an example how the Semantic Web helps NLP. However, our systems also generate massive episodic knowledge of the same type as the Semantic Web is built on. We thus envision a cycle of knowledge acquisition and NLP improvement on a massive scale. This article reports on the details of the system but also on the performance of various high-level components. We demonstrate that our system performs at state-of-the-art level for various subtasks in the four languages of the project, but that we also consider the full integration of these tasks in an overall system with the purpose of reading text. We applied our system to millions of news articles, generating billions of triples expressing formal semantic properties. This shows the capacity of the system to perform at an unprecedented scale.},
author = {Vossen, Piek and Agerri, Rodrigo and Aldabe, Itziar and Cybulska, Agata and van Erp, Marieke and Fokkens, Antske and Laparra, Egoitz and Minard, Anne Lyse and {Palmero Aprosio}, Alessio and Rigau, German and Rospocher, Marco and Segers, Roxane},
doi = {10.1016/j.knosys.2016.07.013},
file = {:Users/damian/Dropbox/uva/literatuur-mendeley/Vossen et al.{\_}2016.pdf:pdf},
issn = {09507051},
journal = {Knowledge-Based Systems},
keywords = {Cross-lingual interopearbility,Event extraction,Knowledge resources,Natural language processing,Semantic web},
pages = {60--85},
publisher = {Elsevier B.V.},
title = {{NewsReader: Using knowledge resources in a cross-lingual reading machine to generate more knowledge from massive streams of news}},
url = {http://dx.doi.org/10.1016/j.knosys.2016.07.013},
volume = {110},
year = {2016}
}

@article{Atefeh2015,
author = {Atefeh, Farzindar and Khreich, Wael},
doi = {10.1111/coin.12017},
file = {:Users/damian/Dropbox/uva/literatuur-mendeley/Atefeh, Khreich{\_}2015.pdf:pdf},
issn = {08247935},
journal = {Computational Intelligence},
keywords = {event detection,event identification,microblogs,monitoring social media,twitter data stream},
month = {feb},
number = {1},
pages = {132--164},
title = {{A Survey of Techniques for Event Detection in Twitter}},
volume = {31},
year = {2015}
}

@article{Graus2018,
abstract = {We study how collective memories are formed online. We do so by tracking entities that emerge in public discourse, that is, in online text streams such as social media and news streams, before they are incorporated into Wikipedia, which, we argue, can be viewed as an online place for collective memory. By tracking how entities emerge in public discourse, i.e., the temporal patterns between their first mention in online text streams and subsequent incorporation into collective memory, we gain insights into how the collective remembrance process happens online. Specifically, we analyze nearly 80,000 entities as they emerge in online text streams before they are incorporated into Wikipedia. The online text streams we use for our analysis comprise of social media and news streams, and span over 579 million documents in a timespan of 18 months. We discover two main emergence patterns: entities that emerge in a "bursty" fashion, i.e., that appear in public discourse without a precedent, blast into activity and transition into collective memory. Other entities display a "delayed" pattern, where they appear in public discourse, experience a period of inactivity, and then resurface before transitioning into our cultural collective memory.},
archivePrefix = {arXiv},
arxivId = {1701.04039},
author = {Graus, David and Odijk, Daan and de Rijke, Maarten},
doi = {10.1002/asi.24004},
eprint = {1701.04039},
file = {:Users/damian/Dropbox/uva/literatuur-mendeley/Graus, Odijk, de Rijke{\_}2018.pdf:pdf},
issn = {23301643},
journal = {Journal of the Association for Information Science and Technology},
number = {6},
pages = {773--786},
title = {{The birth of collective memories: Analyzing emerging entities in text streams}},
volume = {69},
year = {2018}
}

@article{Kusner2015,
abstract = {We present the Word Mover's Distance (WMD), a novel distance function between text docu-ments. Our work is based on recent results in word embeddings that learn semantically mean-ingful representations for words from local co-occurrences in sentences. The WMD distance measures the dissimilarity between two text doc-uments as the minimum amount of distance that the embedded words of one document need to " travel " to reach the embedded words of another document. We show that this distance metric can be cast as an instance of the Earth Mover's Dis-tance, a well studied transportation problem for which several highly efficient solvers have been developed. Our metric has no hyperparameters and is straight-forward to implement. Further, we demonstrate on eight real world document classi-fication data sets, in comparison with seven state-of-the-art baselines, that the WMD metric leads to unprecedented low k-nearest neighbor docu-ment classification error rates.},
author = {Kusner, Matt J and Sun, Yu and Kolkin, Nicholas I and Weinberger, Kilian Q},
file = {:Users/damian/Dropbox/uva/literatuur-mendeley/Kusner et al.{\_}2015.pdf:pdf},
isbn = {9781510810587},
issn = {1938-7228},
journal = {Proceedings of The 32nd International Conference on Machine Learning},
pages = {957--966},
pmid = {1714571},
title = {From Word Embeddings To Document Distances},
volume = {37},
year = {2015}
}

@book{Shoemaker1996,
address = {White Plains},
author = {Shoemaker, Pamela J. and Reese, Stephen D.},
edition = {2nd},
file = {::},
publisher = {Longman},
title = {{Mediating the Message: Theories of Influences on Mass Media Content}},
year = {1996}
}


@article{Larsen1954,
author = {Larsen, Otto N and Hill, Richard J},
file = {:Users/damian/Dropbox/uva/literatuur-mendeley/Larsen, Hill{\_}1954.pdf:pdf},
journal = {American Sociological Review},
number = {4},
pages = {426--433},
title = {Mass Media and Interpersonal Communication in the Diffusion of a News Event},
volume = {19},
year = {1954}
}

@misc{vermeer2018, 
title={A supervised machine learning method to classify Dutch-language news items},  DOI={10.6084/m9.figshare.7314896.v1}, 
author={Vermeer, Susan}, 
year={2018}
}

@book{Bird:2009:NLP:1717171,
 author = {Bird, Steven and Klein, Ewan and Loper, Edward},
 title = {Natural Language Processing with Python},
 year = {2009},
 isbn = {0596516495, 9780596516499},
 edition = {1st},
 publisher = {O'Reilly Media, Inc.},
} 

@article{sidorov2014soft,
  title={Soft similarity and soft cosine measure: Similarity of features in vector space model},
  author={Sidorov, Grigori and Gelbukh, Alexander and G{\'o}mez-Adorno, Helena and Pinto, David},
  journal={Computaci{\'o}n y Sistemas},
  volume={18},
  number={3},
  pages={491--504},
  year={2014},
  publisher={Centro de Investigaci{\'o}n en Computaci{\'o}n, IPN}
}


@misc{gensimsoftcosine,
title = {Finding similar documents with Word2Vec and Soft Cosine Measure},
author = {{RaRe Technologies}},
year = {2018},
url = {https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/soft\_cosine\_tutorial.ipynb}

}

@inproceedings{gensim,
address = {Valletta, Malta},
annote = {$\backslash$url{\{}http://is.muni.cz/publication/884893/en{\}}},
author = {Rehurek, Radim and Sojka, Petr},
booktitle = {Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks},
pages = {45--50},
publisher = {ELRA},
title = {{Software Framework for Topic Modelling with Large Corpora}},
year = {2010}
}



@inproceedings{mikolov2013distributed,
  title={Distributed representations of words and phrases and their compositionality},
  author={Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
  booktitle={Advances in neural information processing systems},
  pages={3111--3119},
  year={2013}
}

@article{boydstun2014two,
  title={Two faces of media attention: Media storm versus non-storm coverage},
  author={Boydstun, Amber E and Hardy, Anne and Walgrave, Stefaan},
  journal={Political Communication},
  volume={31},
  number={4},
  pages={509--531},
  year={2014},
  publisher={Taylor \& Francis}
}


@article{Moeller2016,
abstract = {{\textcopyright} 2016, {\textcopyright} Judith Moeller, Damian Trilling, Natali Helberger, Kristina Irion, Claes de Vreese. Purpose: This paper aims to shed light on the impact of personalized news media on the shared issue agenda that provides democracies with a set of topics that structure the public debate. The advent of personalized news media that use smart algorithms to tailor the news offer to the user challenges the established way of setting the agenda of such a common core of issues. Design/methodology/approach: This paper tests the effects of personalized news use on perceived importance of these issues in the common core. In particular, the authors study whether personalized news use leads to a concentration at the top of the issue agenda or to a more diverse issue agenda with a long tail of topics. Findings: Based on a cross-sectional survey of a representative population sample (n = 1,556), we find that personalized news use does not lead to a small common core in which few topics are discussed extensively, yet there is a relationship between personalized news use and a preference for less discussed topics. This is a result of a specific user profile of personalized news users: younger, more educated news users are more interested in topics at the fringes of the common core and also make more use of personalized news offers. Research limitations/implications: The results are discussed in the light of media diversity and recent advances in public sphere research. Originality/value: This paper contributes to the ongoing debate about algorithmic news dissemination. While, currently, much attention is reserved for the role of platforms as information gatekeepers in relationship to the news media, maybe their ability to enable or hinder the audience in discovering and distributing news content is part of what really characterizes their influence on the market place of ideas.},
author = {Moeller, Judith and Trilling, Damian and Helberger, Natali and Irion, Kristina and de Vreese, Claes H.},
doi = {10.1108/info-05-2016-0020},
file = {:Users/damian/Dropbox/uva/literatuur-mendeley/Moeller et al.{\_}2016(3).pdf:pdf},
issn = {14636697},
journal = {info},
keywords = {,Communication technologies,Statistical analysis,Television news,Worldwide web},
month = {aug},
number = {6},
pages = {26--41},
title = {{Shrinking core? Exploring the differential agenda setting power of traditional and personalized news media}},
volume = {18},
year = {2016}
}

@article{Chaffee2001,
author = {Chaffee, Steven H. and Metzger, Miriam J.},
doi = {10.1207/S15327825MCS0404\_3},
file = {:Users/damian/Dropbox/uva/literatuur-mendeley/Chaffee, Metzger{\_}2001.pdf:pdf},
issn = {1520-5436},
journal = {Mass Communication and Society},
mendeley-groups = {papers/damianpetro},
number = {4},
pages = {365--379},
title = {{The end of mass communication?}},
volume = {4},
year = {2001}
}

@article{Ferree2002,
abstract = {The purpose of this article is to review four traditions of democratic theory, mining them for answers they suggest for the public sphere and more particularly, for mass media discourse in "actually existing democracies." Researchers regard this categorization as a convenient organizing tool for attempting to identify normative criteria that play a significant role within and across perspectives. A number of writers overlap traditions or make shifts over time, so one may consider their ideas wherever it seems most convenient. Researchers label the four traditions as Representative Liberal, Participatory Liberal, Discursive, and Constructionist. In each of these traditions sketched below, researchers attempt to highlight ideas seen as being shared, thus defining a tradition and to highlight the specific normative criteria that each perspective would endorse and emphasize. At the end, the article summarizes these criteria in terms of who should speak, the content of the process, style of speech preferred and the relationship between discourse and decision making that is sought. Finally, one compares the hierarchy of values expressed by each tradition and briefly report some findings of the empirical study that has been undertaken and that suggests that Germany and the United States each conforms more closely to standards of different traditions.},
author = {Ferree, Myra Marx and Gamson, William A and Gerhards, Rgen and Rucht, Dieter},
doi = {10.1023/A:1016284431021},
file = {:Users/damian/Dropbox/uva/literatuur-mendeley/Ferree et al.{\_}2002.pdf:pdf},
issn = {03042421},
journal = {Theory and Society},
number = {3},
pages = {289--324},
publisher = {Springer},
title = {Four models of the public sphere in modern democracies},
volume = {31},
year = {2002}
}

﻿@article{Stromback2005,
abstract = {The literature discussing the impact of media and journalism upon democracy, typically criticizes both media and journalism for their content and their negative effects on some aspects of democracy. In turn, this raises the question of identifying news standards by which the quality of news journalism might be evaluated. But neither the proposed news standards nor the criticism levelled against them specify with sufficient clarity the model of democracy to be used as a normative departure. This article argues that the question of proper news standards cannot be addressed in isolation from the question of different normative models of democracy. In order to discover news standards by which the quality of news journalism can or should be evaluated, it analyzes four normative models of democracy and their demands upon citizens: procedural democracy, competetive democracy, participatory democracy and deliberative democracy. Building upon that analysis, the article asks: What normative implications for media and news journalism follow from the distinctive perspectives of procedural, competitive, participatory and deliberative democracy?},
author = {Str{\"{o}}mb{\"{a}}ck, Jesper},
doi = {10.1080/14616700500131950},
file = {:Users/damian/Dropbox/uva/literatuur-mendeley/Str{\"{o}}mb{\"{a}}ck{\_}2005.pdf:pdf},
isbn = {1461670050013},
issn = {1461-670X},
journal = {Journalism Studies},
number = {3},
pages = {331--345},
publisher = {Routledge, part of the Taylor {\&} Francis Group},
title = {In Search of a Standard: four models of democracy and their normative implications for journalism},
volume = {6},
year = {2005}
}


@article{Zaller2003,
abstract = {Scholarly evaluations of the quality of news are often negative, sometimes scathingly so. The rise of soft news and market-driven journalism in recent years has increased the intensity of this criticism. This article argues, however, that much criticism of news is based on an ideal of citizenship and a standard of quality that are neither realistic nor necessary for the functioning of democracy. The article therefore pro- poses a new, less demanding standard of quality and defends it as adequate to the informational needs of citizens in a democracy.},
archivePrefix = {arXiv},
arxivId = {429},
author = {Zaller, John},
doi = {10.1080/10584600390211136},
eprint = {429},
file = {:Users/damian/Dropbox/uva/literatuur-mendeley/Zaller{\_}2003.pdf:pdf},
isbn = {1058-4609},
issn = {10584609},
journal = {Political Communication},
keywords = {Mass media,News standards,Political information,Soft news},
number = {2},
pages = {109--130},
title = {{A new standard of news quality: Burglar alarms for the monitorial citizen}},
volume = {20},
year = {2003}
}


@article{Staab1990,
author = {Staab, Joachim Friedrich},
journal = {European Journal of Communication},
pages = {423--443},
title = {The role of news factors in news selection: A theoretical reconsideration},
volume = {5},
year = {1990}
}


@incollection{Geiss2018,
	author = {Gei{\ss}, Stefan},
	title = {The dynamics of media attention to issues: {T}owards standardizing measures, dimensions, and profiles},
	doi = {10.2307/j.ctt21215m0},
	year = 2018,
	publisher = {Amsterdam University Press},
	editor = {Peter Vasterman},
	booktitle = {From Media Hype to Twitter Storm: {N}ews explosions and their impact on issues, crises and public opinion},
	pages = {83--113}
}

@article{Baumgartner2006,
abstract = {Abstract Studying agenda-setting and policy dynamics is a well-established research tradition dating back to the work of Bachrach and Baratz and Schattschneider. The research tradition provides considerable insights into how changes in agendas and political attention affect public policy. However, the research tradition has been strongly dominated by studies of the US and has suffered from a lack of comparative studies. This paper discusses the different ways in which such comparative studies can be conducted as well as the potential insights which may be gained from them.},
author = {Baumgartner, Frank R. and Green-Pedersen, Christoffer and Jones, Bryan D.},
doi = {10.1080/13501760600923805},
file = {:Users/damian/Dropbox/uva/literatuur-mendeley/Baumgartner, Green-Pedersen, Jones{\_}2006.pdf:pdf},
isbn = {1350176060},
issn = {1350-1763},
journal = {Journal of European Public Policy},
number = {7},
pages = {959--974},
title = {{Comparative studies of policy agendas}},
volume = {13},
year = {2006}
}

@incollection{Guo2013,
address = {New York, NY},
author = {Guo, L},
booktitle = {Agenda setting in a 2.0 world: New agendas in communication},
editor = {Johnson, T.},
pages = {112--133},
publisher = {Routledge},
title = {{Toward the third level of agenda setting theory: A network agenda setting model}},
year = {2013}
}


@inproceedings{GDELT,
abstract = {GDELT—Global Data on Events, Location and Tone—is a new CAMEO-coded data set containing more than 200-million geolocated events with global coverage for 1979 to the present. The data are based on news reports from a variety of international news sources coded using the Tabari system for events and additional software for location and tone. The data is freely available and we expect to provide daily updates. This paper describes the news sources and some of their characteristics, the various pro- cessing steps that are used in generating the data, some comparisons with the KEDS Levants/Reuters and ICEWS/Asia data sets, and some visualizations. We conclude with an outline of planned enhancements to the data in the near future: these include recoding with new WordNet-enhanced dictionaries, the extension of the CAMEO cod- ing to incorporate codes for financial events, disease outbreaks and natural disasters, and the development of an open-source Python-based successor to Tabari which will use parsed input from existing natural language processing tools.},
author = {Schrodt, Philip A and Leetaru, Kalev},
booktitle = {International Studies Association Meeting},
file = {:Users/damian/Dropbox/uva/literatuur-mendeley/Schrodt, Leetaru{\_}2013.pdf:pdf},
title = {{GDELT}: Global Data on Events, Location and Tone, 1979-2012},
year = {2013}
}


@article{Buhl2016,
abstract = {According to previous research, online news production is characterized by the volatility and fast pace of reporting as well as by the interplay of various contributors to news flows within the online news ecosystem. However, as both concepts are challenging to study, research discovering the dynamics of news flows at the ecosystem level is rare. Utilizing techniques of automated content analysis and big databases of online news content, this paper proposes an approach to make the pace and dynamics of news diffusion processes among online news sites observable. It reviews several lines of argument for the fast pace of online news production based on “breaking news” coverage, immediacy as a norm, and imitation among online news providers. Applying an analytical framework from diffusion of news events research, we find recurring dynamics for the diffusion processes of 95 events among 28 German online news sites. We distinguish three clusters of diffusion processes based on their temporal patterns. On average, it takes only 1.5 hours for a majority of online news sites to report news of the 43 events in the largest cluster. The narrow timing of broadly shared news decisions within the ecosystem reveals a strong potential for abrupt surges or bursts in online news flows.},
author = {Buhl, Florian and G{\"{u}}nther, Elisabeth and Quandt, Thorsten},
doi = {10.1080/1461670X.2016.1168711},
file = {:Users/damian/Dropbox/uva/literatuur-mendeley/Buhl, G{\"{u}}nther, Quandt{\_}2018.pdf:pdf},
issn = {1461-670X},
journal = {Journalism Studies},
number = {1},
pages = {79--104},
title = {{Observing the dynamics of the online news ecosystem}},
volume = {19},
year = {2018}
}

@article{Haim2018a,
abstract = {Since the early introduction of the notion of agenda-setting, researchers have attempted to determine the factors that shape media agendas. One of the key sources of media agenda has been identified as intermedia flow, which various studies revealed in the offline-to-online-to-SNS media context. While most of them focused on the offline-online flow, the present study examines agenda-setting within the new online platforms in various countries, thus allowing for crosscountry and cross-media comparisons. We applied time-series analysis to new online media and to traditional online media samples in the context of Edward Snowden's NSA revelations. Our findings of intermedia agenda-setting effects show a moderate but consistent flow from new online media to traditional online media. This highlights the importance of studying these new directions of agenda flow. Apart from that, no profound agenda-setting patterns can be found elsewhere. Possible reasons and implications are discussed.},
author = {Haim, Mario and Weimann, Gabriel and Brosius, Hans-Bernd},
doi = {10.1007/s42001-018-0016-y},
isbn = {0123456789},
issn = {2432-2717},
journal = {Journal of Computational Social Science},
keywords = {Agenda-setting,Internet,Public sphere,Comparative research,a professional scientific editor,agenda-setting,comparative research,he passed away in,internet,last,may 2015,one he edited before,public sphere,this paper is dedicated,this paper was the,to asher goldstein},
number = {2},
pages = {277--294},
publisher = {Springer Singapore},
title = {{Who sets the cyber agenda? Intermedia agenda-setting online: the case of Edward Snowden's NSA revelations}},
volume = {1},
year = {2018}
}


@article{Vasterman2005,
author = {Vasterman, Peter L.M.},
doi = {10.1177/0267323105058254},
file = {:Users/damian/Dropbox/uva/literatuur-mendeley/Vasterman{\_}2005.pdf:pdf},
issn = {0267-3231},
journal = {European Journal of Communication},
keywords = {amplification,framing,journalistic,street violence},
number = {4},
pages = {508--530},
title = {{Media-Hype}: {S}elf-Reinforcing News Waves, Journalistic Standards and the Construction of Social Problems},
volume = {20},
year = {2005}
}


@article{Kepplinger1995,
abstract = {Based on the theoretical distinction between genuine, mediated and staged events, the causes and consequences of waves of news coverage are analysed. It is assumed that key events change the criteria for the selection of news and stimulate new activities which in turn get covered by the media. It is further assumed that, as a consequence, the news coverage creates the false impression of an increasing number of genuine events — occurrences which by definition cannot be influenced by media coverage — even if their actual number remains unchanged. This is due to the fact that the media report more often on past events, give new events more coverage and make both appear very similar to the key event. The theoretical assumptions are analysed with respect to coverage on AIDS, earthquakes and truck accidents before and after three key events.},
author = {Kepplinger, Hans Mathias and Habermeier, Johanna},
doi = {10.1177/0267323195010003004},
file = {:Users/damian/Dropbox/uva/literatuur-mendeley/Kepplinger, Habermeier{\_}1995.pdf:pdf},
issn = {0267-3231},
journal = {European Journal of Communication},
keywords = {agenda-building,key events,news selection,risk communication},
number = {3},
pages = {371--390},
title = {The Impact of Key Events on the Presentation of Reality},
volume = {10},
year = {1995}
}


@article{Hellsten2015,
abstract = {Abstract Purpose – Research into the emergence of a hype requires a mixed methods approach that takes into account both the evolution over time and mutual influences across different types of media. The purpose of this paper is to present a methodological approach to detect an emerging hype in online communications. Design/methodology/approach – The paper combines Auto Regressive Integrated Moving Average (ARIMA) time series modelling and semantic co-word networks, and this combination of methods provides a view on the emergence and development of a hype at the level of mutual influences across a heterogeneous set of newspaper and blog data. The subject scope of the paper is the climategate hype. The climategate hype was triggered by the online publication of a set of hacked e-mails belonging to climate researchers at the East Anglia University in November 2009. Findings – The main findings show that the climategate hype was initiated in the blogs, and the newspapers were reacting to the blogs. At the level of semantics, the blogs and the newspapers framed the issue from opposite perspectives. Research limitations/implications – The combination of methods contributes theoretical insights to how blogs interact with more traditional media on hype generation and methodological insights to internet researchers investigating emergent online hypes. The method calls for further validation. Practical implications – Investigating the emergence and evolution of a hype, and the interaction of the two media is relevant for journalists in becoming more reflexive in their practices and the cues from the outside world. Originality/value – The paper is novel in its combination of the two specific methods, ARIMA time series modelling and co-word networks and its attempt to identify the media origins of a hype, and especially the interaction between blogs and newspapers.},
author = {Hellsten, Iina and Vasileiadou, Eleftheria},
doi = {10.1108/IntR-05-2014-0130},
file = {:Users/damian/Dropbox/uva/literatuur-mendeley/Hellsten, Vasileiadou{\_}2015.pdf:pdf},
issn = {10662243},
journal = {Internet Research},
keywords = {ARIMA modelling,Blog analysis,Climategate,Hype,Newspaper analysis,Semantic co-word networks},
number = {4},
pages = {589--609},
title = {{The creation of the climategate hype in blogs and newspapers: Mixed methods approach}},
volume = {25},
year = {2015}
}


@article{Geiss2018b,
author = {Gei{\ss}, Stefan and Magin, Melanie and Stark, Birgit and J{\"{u}}rgens, Pascal},
doi = {10.5771/1615-634X-2018-4-502},
file = {:Users/damian/Dropbox/uva/literatuur-mendeley/Gei{\ss} et al.{\_}2018.pdf:pdf},
issn = {1615-634X},
journal = {Medien {\&} Kommunikationswissenschaft},
keywords = {allerdings vergr{\"{o}}{\ss}ert die nutzung,als informationsquellen die anschlussf{\"{a}}-,das gilt umso mehr,demnach verringern soziale medien,hatten,higkeit individueller themenhorizonte nicht,je extre-,sie am wichtigsten fanden,sie sich dar{\"{u}}ber unterrichtet,traditionellen nachrichtenmedien die anschlussf{\"{a}}hi,und aus welchen quellen,von},
number = {4},
pages = {502--525},
title = {{„Common Meeting Ground” in Gefahr? Selektionslogiken politischer Informationsquellen und ihr Einfluss auf die Fragmentierung individueller Themenhorizonte}},
volume = {66},
year = {2018}
}


@book{davies2009,
author = {Nick Davies},
title = {Flat Earth News: An Award-Winning Reporter Exposes Falsehood, Distortion and Propaganda in the Global Media},
year = {2009},
publisher = {Vintage},
address = {London}
}


@article{infomap,
abstract = {Comprehending complex systems by simplifying and highlighting important dynamical patterns requires modeling and mapping higher-order network flows. However, complex systems come in many forms and demand a range of representations, including memory and multilayer networks, which in turn call for versatile community-detection algorithms to reveal important modular regularities in the flows. Here we show that various forms of higher-order network flows can be represented in a unified way with networks that distinguish physical nodes for representing a{\~{}}complex system's objects from state nodes for describing flows between the objects. Moreover, these so-called sparse memory networks allow the information-theoretic community detection method known as the map equation to identify overlapping and nested flow modules in data from a range of{\~{}}different higher-order interactions such as multistep, multi-source, and temporal data. We derive the map equation applied to sparse memory networks and describe its search algorithm Infomap, which can exploit the flexibility of sparse memory networks. Together they provide a general solution to reveal overlapping modular patterns in higher-order flows through complex systems.},
author = {Edler, Daniel and Bohlin, Ludvig and Rosvall, Martin},
doi = {10.3390/a10040112},
issn = {19994893},
journal = {Algorithms},
keywords = {Community detection,Higher-order network flows,Infomap,Memory networks,Multilayer networks,Overlapping communities},
title = {Mapping higher-order network flows in memory and multilayer networks with {I}nfomap},
year = {2017}
}


@inproceedings{Novotny2018,
address = {New York, New York, USA},
author = {Novotn{\'{y}}, V{\'{i}}t},
booktitle = {{Proceedings of the 27th ACM International Conference on Information and Knowledge Management -- CIKM '18}},
doi = {10.1145/3269206.3269317},
isbn = {9781450360142},
pages = {1639--1642},
publisher = {ACM Press},
title = {{Implementation notes for the Soft Cosine Measure}},
year = {2018}
}

@conference{AEM,
  title    = {Deriving semantics from Dutch media corpora:
The Amsterdam word embedding model},
  author   = {Anne Kroon and Damian Trilling and Antske Fokkens and Felicia Loecherbach and Judith Moeller and Wouter {van Atteveldt} and Mariken {van der Velden}},
  year     = "2019",
  booktitle = {Etmaal van de Communicatiewetenschap},
  address = {Nijmegen, Netherlands}
  }
  
 @incollection{Marcinkowski2008,
address = {Chichester, UK},
author = {Marcinkowski, Frank},
booktitle = {The International Encyclopedia of Communication},
doi = {10.1002/9781405186407.wbiecp138},
file = {:home/damian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Marcinkowski - 2017 - Public Sphere, Fragmentation of.pdf:pdf},
publisher = {John Wiley {\&} Sons, Ltd},
title = {Public Sphere, Fragmentation of},
year = {2008}
}


@article{Dimmick2004,
author = {Dimmick, J and Chen, Y and Li, Z},
file = {:home/damian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dimmick, Chen, Li - 2004 - Competition between the Internet and traditional news media The gratification-opportunities niche dimension.pdf:pdf},
journal = {The Journal of Media Economics},
number = {1},
pages = {19--33},
title = {{Competition between the Internet and traditional news media: The gratification-opportunities niche dimension}},
volume = {17},
year = {2004}
}


@thesis{VanHoof2019,
title = {Fake news contamination? {A} study of agenda setting interaction between fake news and tradtional media},
author = {Marieke {Van Hoof}},
year = {2019},
school = {Universiteit van Amsterdam}
}

@article{Vargo2018,
author = {Vargo, Chris J and Guo, Lei and Amazeen, Michelle A},
doi = {10.1177/1461444817712086},
file = {:Users/damian/Dropbox/uva/literatuur-mendeley/Vargo, Guo, Amazeen{\_}2018.pdf:pdf},
isbn = {1461444817712},
issn = {1461-4448},
journal = {New Media {\&} Society},
keywords = {big data,computational social science,corresponding author,fact-checking,fake news,intermedia agenda,journalism,misinformation,network agenda setting,partisanship,setting},
month = {may},
number = {5},
pages = {2028--2049},
title = {{The agenda-setting power of fake news: A big data analysis of the online media landscape from 2014 to 2016}},
volume = {20},
year = {2018}
}


@article{Mukerjee2017,
abstract = {How do people consume news online? Here, we propose a novel way to answer this question using the browsing behavior of web users and the networks they form while navigating news content. In these networks, two news outlets are connected if they share a fraction of their audiences. We propose two crucial improvements to the methodology employed in previous research: a statistical test to filter out non-significant overlap between sites; and a thresholding approach to identify the core of the audience network. We explain why our approach is better than previous approaches using two data sets: one tracks digital news consumption during the 2016 Brexit referendum in the UK and the other during the 2016 Presidential Election in the US. We show that our filtering technique produces a completely different ranking of top sites, uncovering structural properties in the audience network that would go unnoticed otherwise.},
author = {Mukerjee, Subhayan and Maj{\'{o}}-V{\'{a}}zquez, S{\'{i}}lvia and Gonz{\'{a}}lez-Bail{\'{o}}n, Sandra},
doi = {10.1093/joc/jqx007},
file = {:Users/damian/Dropbox/uva/literatuur-mendeley/Mukerjee, Maj{\'{o}}-V{\'{a}}zquez, Gonz{\'{a}}lez-Bail{\'{o}}n{\_}2018.pdf:pdf},
issn = {0021-9916},
journal = {Journal of Communication},
keywords = {centralization,digital-born,fragmentation,legacy media,network analysis,news,web},
number = {1},
pages = {26--50},
title = {Networks of Audience Overlap in the Consumption of Digital News},
volume = {68},
year = {2018}
}


@incollection{Trillingnetwork,
author = "Damian Trilling",
title = {Conceptualizing and measuring news exposure as network of users and news items},
booktitle = {Measuring media use and exposure: {R}ecent developments and challenges},
editor= {Christina Peter and Theresa K. Naab and Rinaldo K{\"u}hne},
year = {2019},
series={Methoden und Forschungslogik der Kommunikationswissenschaft},
isbn={9783869622873},
publisher={Herbert von Halem}
}

@techreport{Menchen-Trevino2016,
abstract = {Research analyzing real-world web browsing data has generally been collected from digital service providers or the online panelists of corporate research panels. These approaches limit the replicability and the kind of work that can be done. Web Historian's first tool, a Chrome browser extension addresses this problem by enabling researchers to securely collect web browsing history data from participants with a robust informed consent process, and direct benefits to participants. Data visualizations of web browsing history inform participants of what they are submitting to the research project and help them gain further knowledge of their own browsing habits. Web Historian uses data that are already on the user's computer. Participants can submit up to 90 days of browsing history within just a few minutes. Since researchers can recruit participants themselves web browsing history data can be added to other forms of data collection, qualitative or quantitative. The visualizations can also be used for educational purposes.},
author = {Menchen-Trevino, Ericka},
doi = {10.9776/16611},
file = {:Users/damian/Dropbox/uva/literatuur-mendeley/Menchen-Trevino{\_}2016.pdf:pdf},
keywords = {digital trace data,methodology,mixed-methods},
pages = {1--5},
title = {{Web Historian: Enabling multi-method and independent research with real-world web browsing history data}},
year = {2016}
}


@inproceedings{Castillo2014,
abstract = {This paper presents a study of the life cycle of news articles posted online. We describe the interplay between website visitation patterns and social media reactions to news content. We show that we can use this hybrid observation method to characterize distinct classes of articles. We also find that social media reactions can help predict future visitation patterns early and accurately. We validate our methods using qualitative analysis as well as quantitative analysis on data from a large international news network, for a set of articles generating more than 3,000,000 visits and 200,000 social media reactions. We show that it is possible to model accurately the overall traffic articles will ultimately receive by observing the first ten to twenty minutes of social media reactions. Achieving the same prediction accuracy with visits alone would require to wait for three hours of data. We also describe significant improvements on the accuracy of the early prediction of shelf-life for news stories.},
address = {Baltimore, ML},
archivePrefix = {arXiv},
arxivId = {1304.3010},
author = {Castillo, Carlos and El-Haddad, Mohammed and Pfeffer, J{\"{u}}rgen and Stempeck, Matt},
booktitle = {Proceedings of the 17th ACM conference on Computer supported cooperative work {\&} social computing (CSCW '14)},
doi = {10.1145/2531602.2531623},
eprint = {1304.3010},
file = {:Users/damian/Dropbox/uva/literatuur-mendeley//Castillo et al.{\_}2014.pdf:pdf},
mendeley-groups = {trackthenews,papers/damianpetro},
pages = {211--223},
publisher = {ACM},
title = {Characterizing the Life Cycle of Online News Stories Using Social Media Reactions},
year = {2014}
}


@article{Traag2019,
abstract = {Community detection is often used to understand the structure of large and complex networks. One of the most popular algorithms for uncovering community structure is the so-called Louvain algorithm. We show that this algorithm has a major defect that largely went unnoticed until now: the Louvain algorithm may yield arbitrarily badly connected communities. In the worst case, communities may even be disconnected, especially when running the algorithm iteratively. In our experimental analysis, we observe that up to 25{\%} of the communities are badly connected and up to 16{\%} are disconnected. This may present serious issues in subsequent analyses. To address this problem, we introduce the Leiden algorithm. We prove that the Leiden algorithm yields communities that are guaranteed to be connected. In addition, we prove that, when the Leiden algorithm is applied iteratively, it converges to a partition in which all subsets of all communities are locally optimally assigned. Furthermore, by relying on a fast local move approach, the Leiden algorithm runs faster than the Louvain algorithm. We demonstrate the performance of the Leiden algorithm for several benchmark and real-world networks. We find that the Leiden algorithm is faster than the Louvain algorithm and uncovers better partitions, in addition to providing explicit guarantees. Based on our results, we conclude that the Leiden algorithm is preferable to the Louvain algorithm.},
author = {Traag, V. A. and Waltman, L. and van Eck, N. J.},
doi = {10.1038/s41598-019-41695-z},
file = {:Users/damian/Dropbox/uva/literatuur-mendeley/Traag, Waltman, van Eck{\_}2019.pdf:pdf},
isbn = {4159801941695},
issn = {20452322},
journal = {Scientific Reports},
number = {1},
pages = {1--12},
title = {{From Louvain to Leiden: guaranteeing well-connected communities}},
volume = {9},
year = {2019}
}


@article{Blondel2008,
	doi = {10.1088/1742-5468/2008/10/p10008},
	year = 2008,
	publisher = {{IOP} Publishing},
	volume = {2008},
	number = {10},
	pages = {P10008},
	author = {Vincent D Blondel and Jean-Loup Guillaume and Renaud Lambiotte and Etienne Lefebvre},
	title = {Fast unfolding of communities in large networks},
	journal = {Journal of Statistical Mechanics: Theory and Experiment}
}


@article{Traag2015,
	doi = {10.1103/physreve.92.022816},
	year = {2015},
	publisher = {American Physical Society ({APS})},
	volume = {92},
	number = {2},
	author = {V. A. Traag and R. Aldecoa and J.-C. Delvenne},
	title = {Detecting communities using asymptotical surprise},
	journal = {Physical Review E}
}


@article{hopp2019,
  title={{iCoRe}: The {GDELT} Interface for the Advancement of Communication Research},
  author={Hopp, Frederic R and Schaffer, James and Fisher, Jacob T and Weber, Ren{\'e}},
  year={2019},
  journal={Computational Communication Research},
  volume = {1}
}

@inproceedings{Rayson2000,
abstract = {This paper describes a method of comparing corpora which uses frequency profiling. The method can be used to discover key words in the corpora which differentiate one corpus from another. Using annotated corpora, it can be applied to discover key grammatical or word-sense categories. This can be used as a quick way in to find the differences between the corpora and is shown to have applications in the study of social differentiation in the use of English vocabulary, profiling of learner English and document analysis in the software engineering process.},
author = {Rayson, Paul and Garside, Roger},
booktitle = {Proceedings of the workshop on Comparing Corpora},
doi = {10.3115/1117729.1117730},
file = {:home/damian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rayson, Garside - 2000 - Comparing corpora using frequency profiling.pdf:pdf},
keywords = {QA75 Electronic computers. Computer science},
title = {{Comparing corpora using frequency profiling}},
year = {2000}
}

